{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAYTRdL7n2XLHJiDtH9VfS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivanshudwivedi/Deep-Learning/blob/main/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYLmjsP4i5Yo",
        "outputId": "a03d572d-72c2-4238-be81-bec08b627bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan3'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 212 (delta 0), reused 1 (delta 0), pack-reused 207\u001b[K\n",
            "Receiving objects: 100% (212/212), 4.17 MiB | 23.61 MiB/s, done.\n",
            "Resolving deltas: 100% (98/98), done.\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n"
          ]
        }
      ],
      "source": [
        "#Installing StyleGan3\n",
        "!git clone https://github.com/NVlabs/stylegan3.git\n",
        "!pip install ninja\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Stylegan3\n",
        "!python /content/stylegan3/gen_images.py \\\n",
        " --network = https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq=1024x1024.pkl \\\n",
        " --outdir = /content/results --seeds=6600-6625"
      ],
      "metadata": {
        "id": "1DsF1e7Tr3rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running StyleGan with Python\n",
        "import sys\n",
        "sys.path.insert(0, '/content/stylegan3')\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display\n",
        "import torch\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "\n",
        "\n",
        "#Defining the functions needed later\n",
        "def seed2vec(G, seed):\n",
        "  return np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "\n",
        "def display_image(image):\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "def generate_image(G, z, truncation_psi):\n",
        "  Gs_kwargs = {'output_transform' : dict(func = tflib.convert_images_to_uint8, nchw_to_nhwc = True), 'randomize_noise' : False}\n",
        "  if truncation_psi is not None:\n",
        "    Gs_kwargs[truncation_psi] = truncation_psi\n",
        "    label = np.zeros([1] + G.input_shapes[1][1:])\n",
        "    images = G.run(z, label, **G_kwargs)\n",
        "    return images[0]\n",
        "\n",
        "def get_label(G, device, class_idx):\n",
        "  label = torch.zeros([1, G.c_dim], device = device)\n",
        "\n",
        "  if G.c_dim != 0:\n",
        "    if class_idx is not None:\n",
        "      ctx.fail('Must specify class label when using conditional network')\n",
        "    label[:, class_idx] = 1\n",
        "\n",
        "  else:\n",
        "    if class_idx is not None:\n",
        "      print('Warning: class label ignored when running conditonal network')\n",
        "\n",
        "  return label\n",
        "\n",
        "def expand_seed(seeds, vector_size):\n",
        "  result = []\n",
        "\n",
        "  for seed in seeds:\n",
        "    rnd = np.random.RandomState(state)\n",
        "    result.append(rnd.randn(1, vector_size))\n",
        "  return result\n",
        "\n",
        "\n",
        "def generate_image(device, G, z, truncation_psi = 1.0, noise_mode = 'const', class_idx = None):\n",
        "  z = torch.from_numpy(z).to(device)\n",
        "  label = get_label(G, device, class_idx)\n",
        "  img = G(z, label, truncation_psi = truncation_psi, noise_mode = noise_mode)\n",
        "  img = (img.permute(0, 2, 3, 1)*127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "URL = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl'\n",
        "\n",
        "print('Loading networks from : ', URL)\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(URL) as f:\n",
        "  G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
        "\n",
        "#Creating own starting and ending seed\n",
        "SEED_FROM = 1000\n",
        "SEED_TO = 1003\n",
        "\n",
        "#Generate the images for the seeds\n",
        "for i in range(SEED_FROM, SEED_TO):\n",
        "  print('Seed: ', i)\n",
        "  z = seed2vec(G, i)\n",
        "  img = generate_image(device, G, z)\n",
        "  display_image(img)\n",
        "\n",
        "#Transforming the latent vector\n",
        "vector_size = G.z_dim\n",
        "seeds = expand_seed([8192 + 1, 8192 + 9], vector_size)\n",
        "print(seeds[0].shape)\n"
      ],
      "metadata": {
        "id": "aIfYK93tWo-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a video\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/stylegan3')\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display\n",
        "import torch\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "\n",
        "\n",
        "#Defining the functions needed later\n",
        "def seed2vec(G, seed):\n",
        "  return np.random.RandomState(seed).randn(1, G.z_dim)\n",
        "\n",
        "def display_image(image):\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.show()\n",
        "\n",
        "def generate_image(G, z, truncation_psi):\n",
        "  Gs_kwargs = {'output_transform' : dict(func = tflib.convert_images_to_uint8, nchw_to_nhwc = True), 'randomize_noise' : False}\n",
        "  if truncation_psi is not None:\n",
        "    Gs_kwargs[truncation_psi] = truncation_psi\n",
        "    label = np.zeros([1] + G.input_shapes[1][1:])\n",
        "    images = G.run(z, label, **G_kwargs)\n",
        "    return images[0]\n",
        "\n",
        "def get_label(G, device, class_idx):\n",
        "  label = torch.zeros([1, G.c_dim], device = device)\n",
        "\n",
        "  if G.c_dim != 0:\n",
        "    if class_idx is not None:\n",
        "      ctx.fail('Must specify class label when using conditional network')\n",
        "    label[:, class_idx] = 1\n",
        "\n",
        "  else:\n",
        "    if class_idx is not None:\n",
        "      print('Warning: class label ignored when running conditonal network')\n",
        "\n",
        "  return label\n",
        "\n",
        "def expand_seed(seeds, vector_size):\n",
        "  result = []\n",
        "\n",
        "  for seed in seeds:\n",
        "    rnd = np.random.RandomState(state)\n",
        "    result.append(rnd.randn(1, vector_size))\n",
        "  return result\n",
        "\n",
        "\n",
        "def generate_image(device, G, z, truncation_psi = 1.0, noise_mode = 'const', class_idx = None):\n",
        "  z = torch.from_numpy(z).to(device)\n",
        "  label = get_label(G, device, class_idx)\n",
        "  img = G(z, label, truncation_psi = truncation_psi, noise_mode = noise_mode)\n",
        "  img = (img.permute(0, 2, 3, 1)*127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "  return PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "URL = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl'\n",
        "\n",
        "print('Loading networks from : ', URL)\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(URL) as f:\n",
        "  G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
        "\n",
        "#Creating the video\n",
        "SEEDS = [6624, 6618, 6616]\n",
        "STEPS = 100\n",
        "\n",
        "!rm /content/results/*\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "os.makedirs('*./results/', exist_ok = True)\n",
        "\n",
        "#Generate the images for the video\n",
        "idx = 0\n",
        "\n",
        "for i in range(len(SEEDS) - 1):\n",
        "  v1 = seed2vec(G, SEEDS[i])\n",
        "  v2 = seed2vec(G, SEEDS[i+1])\n",
        "\n",
        "  diff = v2 - v1\n",
        "  step = diff/STEPS\n",
        "  current = v1.copy()\n",
        "\n",
        "  for j in tqdm(range(STEPS), desc = f'Seed {SEEDS[i]}'):\n",
        "    current += step\n",
        "    img = generate_image(device, G, current)\n",
        "    img.save()\n",
        "    idx += 1\n",
        "!ffmpeg -r 3- -i /content/results/frame-%d.png - vcodec mpeg4 -y movie.mp4\n",
        "\n",
        "#Finetuning an image\n",
        "START_SEED = 4022\n",
        "current = seed2vec(G, START_SEED)\n",
        "\n",
        "img = generate_image(device, G, current)\n",
        "SCALE = 0.5\n",
        "display_image(img)\n",
        "\n",
        "EXPLORE_SIZE = 25\n",
        "explore = []\n",
        "\n",
        "for i in range(EXPLORE_SIZE):\n",
        "  explore.append(np.random.randn(1, 512) - 0.5)\n",
        "\n",
        "#Choose direction to move\n",
        "MOVE_DIRECTION = -1\n",
        "SCALE = 0.5\n",
        "\n",
        "if MOVE_DIRECTION >= 0:\n",
        "  current += explore[MOVE_DIRECTION]\n",
        "\n",
        "for i, mv in enumerate(explore):\n",
        "  print('Direction', i)\n",
        "  z = current + mv\n",
        "  img = generate_image(device, G, z)\n",
        "  display_image(img)\n",
        "\n"
      ],
      "metadata": {
        "id": "uf6RYEbTeEg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic GAN in Keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dropout, Reshape, Dense\n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Downloading the files to drive\n",
        "DOWNLOAD_SOURCE = 'https://people.ucsc.edu/~cchakrab/data/image/image_archive.zip'\n",
        "DOWNLOAD_PATH = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfing('/') + 1:]\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  PATH = '/content/'\n",
        "  EXTRACT_TARGET = os.path.join(PATH, 'faces')\n",
        "  SOURCE = os.path.join(PATH, '/content/faces/faces')\n",
        "  TARGET = os.path.join(PATH, '/content/faces-processed')\n",
        "\n",
        "else:\n",
        "  PATH = '/Users/sdwivedi'\n",
        "  EXTRACT_TARGET = os.path.join(PATH, 'faces')\n",
        "  SOURCE = os.path.join(PATH, '/content/faces/faces')\n",
        "  TARGET = os.path.join(PATH, '/content/faces-processed')\n",
        "\n",
        "#Generation Resolution\n",
        "GENERATE_RES = 3\n",
        "GENERATE_SQUARE = 32*GENERATE_RES\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "#Preview Image\n",
        "PREVIEW_ROWS = 4\n",
        "PREVIEW_COLS = 7\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "SEED_SIZE = 100\n",
        "\n",
        "DATA_PATH = '/content/faces-processed'\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000\n",
        "\n",
        "print('The pixels of the images produced would be: ', GENERATE_SQUARE)\n",
        "\n",
        "#Training path\n",
        "training_binary_path = os.path.join(DATA_PATH, GENERATE_SQUARE)\n",
        "print('Looking for the file: ', training_binary_path)\n",
        "\n",
        "if not os.path.isfile(training_binary_pitch):\n",
        "  start = time.time()\n",
        "\n",
        "  training_image = []\n",
        "  faces_data = os.path.join(DATA_PATH, 'face_images')\n",
        "  for filename in tqdm(os.listdir(faces_path)):\n",
        "    path = os.path.join(faces_path, filename)\n",
        "    image = Image.open(path).resize((GENERATE_SQUARE, GENERATE_SQUARE), Image.ANTIALIAS)\n",
        "    training_data.append(np.asarray(image))\n",
        "  training_data = np.reshape(training_image, (-1, GENERATE_SQAURE, GENERATE_SQUARE, IMAGE_CHANNELS))\n",
        "  training_data = training_data.astype(np.float32)\n",
        "  training_data = training_data / 127.5 - 1\n",
        "\n",
        "  np.save(training_binary_path, training_path)\n",
        "  elapsed = time.time() - start\n",
        "  print('Processing Time: ', elapsed)\n",
        "  training_data = np.load(training_binary_path)\n",
        "\n",
        "#Batch & Shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "#Building the Generator\n",
        "def build_generator(seed_size, channels):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(4096, activation = 'relu', input_dim = seed_size))\n",
        "  model.add(Reshape(4, 4, 256))\n",
        "\n",
        "  model.add(UpSampling2D())\n",
        "  model.add(Conv2D(256, kernel_size = 3, padding = 'same'))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(Activation = 'relu')\n",
        "\n",
        "  model.add(UpSampling2D())\n",
        "  model.add(Conv2D(256, kernel_size = 3, padding = 'same'))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(Activation = 'relu')\n",
        "\n",
        "  #Output resolution - addtional upsampling\n",
        "  model.add(UpSampling2D())\n",
        "  model.add(Conv2D(128, kernel_size = 3, padding = same))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(Activation = 'relu')\n",
        "\n",
        "  if GENERATE_RES > 1:\n",
        "    model.add(UpSampling2D(size = (GENERATE_RES, GENERATE_RES)))\n",
        "    model.add(Conv2D(128, kernel_size = 3, padding = 'same'))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation = 'relu')\n",
        "\n",
        "  #Final CNN layer\n",
        "  model.add(Conv2D(channels, kernel_size = 3, padding = 'same'))\n",
        "  model.add(Activation('tanh'))\n",
        "\n",
        "  return model\n",
        "\n",
        "#Building the discriminator\n",
        "def build_discriminator(image_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size = 3, strides = 2, input_shape = image_shape, padding = 'shape'))\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(128, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(256, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv2D(512, kernel_size = 3, strides = 2, padding = 'same'))\n",
        "  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\n",
        "  model.add(BatchNormalization(momentum = 0.8))\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def save_images(cnt, noise):\n",
        "  image_array = np.full((\n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQAURE + PREVIEW_MARGIN)),\n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQAURE + PREVIEW_MARGIN)),\n",
        "      ,IMAGE_CHANNELS) 255, dtype = 'uint32'\n",
        "  )\n",
        "  generated_images = generator.predict(noise)\n",
        "  generated_images = 0.5*generated_images + 0.5\n",
        "\n",
        "  image_count = 0\n",
        "  for row in range(PREVIEW_ROWS):\n",
        "    for col in range(PREVIEW_COLS):\n",
        "      r = row * (GENERATE_SQUARE + 16) + PREVIEW_MARGIN\n",
        "      c = col * (GENERATE_SQAURE + 16) + PREVIEW_MARGIN\n",
        "      image_array[r : r + GENERATE_SQUARE, c : c + GENERATE_SQAURE] = generated_images[image_count]*255\n",
        "      image_count += 1\n",
        "\n",
        "  output_path = os.path.join(DATA_PATH, 'output')\n",
        "  if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "  filename = os.path.join(DATA_PATH, f'train-{cnt}.png')\n",
        "  im = Image.fromarray(image_array)\n",
        "  im.save(filename)\n",
        "\n",
        "#Generate image from generator\n",
        "generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n",
        "noise = tf.random.normal([1, SEED_SIZE])\n",
        "generated_image = generator(noise, training = False)\n",
        "plt.imshow(generate_image[0, : , : , 0])\n",
        "\n",
        "image_shape = (GENERATE_SQUARE, GENERATE_SQAURE, IMAGE_CHANNELS)\n",
        "discriminator = build_discriminator(image_shape)\n",
        "decision = discriminator(generated_image)\n",
        "print(decision)\n",
        "\n",
        "#Defining Loss Functions\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "#Training DataSet\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_images = generator(seed, training = True)\n",
        "\n",
        "    real_output = discriminator(images, training = True)\n",
        "    fake_output = discriminator(generated_images, training = True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, generator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "  return gen_loss, disc_loss\n",
        "\n",
        "def train(dataset, epochs):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n",
        "  start = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    gen_loss_list = []\n",
        "    disc_loss_list =[]\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      t = train_step(image_batch)\n",
        "      gen_loss_list.append(t[0])\n",
        "      disc_loss_list.append(t[1])\n",
        "\n",
        "    g_loss = sum(gen_loss_list)/len(gen_loss_list)\n",
        "    d_loss = sum(disc_loss_list)/len(disc_loss_list)\n",
        "\n",
        "    epoch_elapsed = time.time() - start_time\n",
        "    save_images(epoch, fixed_seed)\n",
        "\n",
        "  elapsed_time = time.time() - start_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "RzbhOhH0nhXI",
        "outputId": "196cdd82-bf6f-4dbc-dfb7-9aaef7bfd7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport tensorflow as tf\\nfrom tensorflow.keras.layers import Input, Dropout, Reshape, Dense\\nfrom tensorflow.keras.layers import Flatten, BatchNormalization\\nfrom tensorflow.keras.layers import Activation, ZeroPadding2D\\nfrom tensorflow.keras.layers import LeakyReLU\\nfrom tensorflow.keras.layers import UpSampling2D, Conv2D\\nfrom tensorflow.keras.models import Sequential, Model, load_model\\nfrom tensorflow.keras.optimizers import Adam\\nimport numpy as np\\nfrom PIL import Image\\nfrom tqdm import tqdm\\nimport os\\nimport time\\nimport matplotlib.pyplot as plt\\n\\n#Downloading the files to drive \\nDOWNLOAD_SOURCE = 'https://people.ucsc.edu/~cchakrab/data/image/image_archive.zip'\\nDOWNLOAD_PATH = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfing('/') + 1:]\\n\\nCOLAB = True\\n\\nif COLAB:\\n  PATH = '/content/'\\n  EXTRACT_TARGET = os.path.join(PATH, 'faces')\\n  SOURCE = os.path.join(PATH, '/content/faces/faces')\\n  TARGET = os.path.join(PATH, '/content/faces-processed')\\n\\nelse:\\n  PATH = '/Users/sdwivedi'\\n  EXTRACT_TARGET = os.path.join(PATH, 'faces')\\n  SOURCE = os.path.join(PATH, '/content/faces/faces')\\n  TARGET = os.path.join(PATH, '/content/faces-processed')\\n\\n#Generation Resolution\\nGENERATE_RES = 3\\nGENERATE_SQUARE = 32*GENERATE_RES\\nIMAGE_CHANNELS = 3\\n\\n#Preview Image\\nPREVIEW_ROWS = 4\\nPREVIEW_COLS = 7\\nPREVIEW_MARGIN = 16\\n\\nSEED_SIZE = 100\\n\\nDATA_PATH = '/content/faces-processed'\\nEPOCHS = 50\\nBATCH_SIZE = 32\\nBUFFER_SIZE = 60000\\n\\nprint('The pixels of the images produced would be: ', GENERATE_SQUARE)\\n\\n#Training path \\ntraining_binary_path = os.path.join(DATA_PATH, GENERATE_SQUARE)\\nprint('Looking for the file: ', training_binary_path)\\n\\nif not os.path.isfile(training_binary_pitch):\\n  start = time.time()\\n\\n  training_image = []\\n  faces_data = os.path.join(DATA_PATH, 'face_images')\\n  for filename in tqdm(os.listdir(faces_path)):\\n    path = os.path.join(faces_path, filename)\\n    image = Image.open(path).resize((GENERATE_SQUARE, GENERATE_SQUARE), Image.ANTIALIAS)\\n    training_data.append(np.asarray(image))\\n  training_data = np.reshape(training_image, (-1, GENERATE_SQAURE, GENERATE_SQUARE, IMAGE_CHANNELS))\\n  training_data = training_data.astype(np.float32)\\n  training_data = training_data / 127.5 - 1\\n\\n  np.save(training_binary_path, training_path)\\n  elapsed = time.time() - start\\n  print('Processing Time: ', elapsed)\\n  training_data = np.load(training_binary_path)\\n\\n#Batch & Shuffle the data\\ntrain_dataset = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\\n\\n#Building the Generator\\ndef build_generator(seed_size, channels):\\n  model = Sequential()\\n  model.add(Dense(4096, activation = 'relu', input_dim = seed_size))\\n  model.add(Reshape(4, 4, 256))\\n\\n  model.add(UpSampling2D())\\n  model.add(Conv2D(256, kernel_size = 3, padding = 'same'))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(Activation = 'relu')\\n\\n  model.add(UpSampling2D())\\n  model.add(Conv2D(256, kernel_size = 3, padding = 'same'))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(Activation = 'relu')\\n\\n  #Output resolution - addtional upsampling\\n  model.add(UpSampling2D())\\n  model.add(Conv2D(128, kernel_size = 3, padding = same))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(Activation = 'relu')\\n\\n  if GENERATE_RES > 1:\\n    model.add(UpSampling2D(size = (GENERATE_RES, GENERATE_RES)))\\n    model.add(Conv2D(128, kernel_size = 3, padding = 'same'))\\n    model.add(BatchNormalization(momentum = 0.8))\\n    model.add(Activation = 'relu')\\n  \\n  #Final CNN layer\\n  model.add(Conv2D(channels, kernel_size = 3, padding = 'same'))\\n  model.add(Activation('tanh'))\\n\\n  return model\\n\\n#Building the discriminator\\ndef build_discriminator(image_shape):\\n  model = Sequential()\\n  model.add(Conv2D(32, kernel_size = 3, strides = 2, input_shape = image_shape, padding = 'shape'))\\n  model.add(LeakyReLU(alpha = 0.2))\\n\\n  model.add(Dropout(0.25))\\n  model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = 'same'))\\n  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(LeakyReLU(alpha = 0.2))\\n\\n  model.add(Dropout(0.25))\\n  model.add(Conv2D(128, kernel_size = 3, strides = 2, padding = 'same'))\\n  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(LeakyReLU(alpha = 0.2))\\n\\n  model.add(Dropout(0.25))\\n  model.add(Conv2D(256, kernel_size = 3, strides = 2, padding = 'same'))\\n  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(LeakyReLU(alpha = 0.2))\\n\\n  model.add(Dropout(0.25))\\n  model.add(Conv2D(512, kernel_size = 3, strides = 2, padding = 'same'))\\n  model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\\n  model.add(BatchNormalization(momentum = 0.8))\\n  model.add(LeakyReLU(alpha = 0.2))\\n\\n  model.add(Dropout(0.25))\\n  model.add(Flatten())\\n  model.add(Dense(1, activation = 'sigmoid'))\\n\\n  return model\\n\\n\\ndef save_images(cnt, noise):\\n  image_array = np.full((\\n      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQAURE + PREVIEW_MARGIN)),\\n      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQAURE + PREVIEW_MARGIN)),\\n      ,IMAGE_CHANNELS) 255, dtype = 'uint32'\\n  )\\n  generated_images = generator.predict(noise)\\n  generated_images = 0.5*generated_images + 0.5\\n\\n  image_count = 0\\n  for row in range(PREVIEW_ROWS):\\n    for col in range(PREVIEW_COLS):\\n      r = row * (GENERATE_SQUARE + 16) + PREVIEW_MARGIN\\n      c = col * (GENERATE_SQAURE + 16) + PREVIEW_MARGIN\\n      image_array[r : r + GENERATE_SQUARE, c : c + GENERATE_SQAURE] = generated_images[image_count]*255\\n      image_count += 1\\n\\n  output_path = os.path.join(DATA_PATH, 'output')\\n  if not os.path.exists(output_path):\\n    os.makedirs(output_path)\\n\\n  filename = os.path.join(DATA_PATH, f'train-{cnt}.png')\\n  im = Image.fromarray(image_array)\\n  im.save(filename)\\n\\n#Generate image from generator \\ngenerator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\\nnoise = tf.random.normal([1, SEED_SIZE])\\ngenerated_image = generator(noise, training = False)\\nplt.imshow(generate_image[0, : , : , 0])\\n\\nimage_shape = (GENERATE_SQUARE, GENERATE_SQAURE, IMAGE_CHANNELS)\\ndiscriminator = build_discriminator(image_shape)\\ndecision = discriminator(generated_image)\\nprint(decision)\\n\\n#Defining Loss Functions \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conditional GANs\n",
        "#Import Libraries\n",
        "\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "from matlplotlib import pyplot\n",
        "\n",
        "#Loading the fashion_mnist dataset\n",
        "from keras.datasets.fashion_mnist import load_data\n",
        "\n",
        "#Load Images to memory\n",
        "(trainX, trainy), (testX, testy) = load_data()\n",
        "\n",
        "print('Train: ', trainX.shape, trainy.shape)\n",
        "print('Test: ', testX.shape, testy.shape)\n",
        "\n",
        "for i in range(100):\n",
        "  pyplot.subplot(10, 10, 1 + i)\n",
        "  pyplot.axis('off')\n",
        "  pyplot.imshow(trainX[i], cmap = 'gray_r')\n",
        "pyplot.show()\n",
        "\n",
        "#Defining the standalone discriminator model\n",
        "def define_discriminator(in_shape = (28, 28, 1), n_classes = 10):\n",
        "  #label input\n",
        "  in_label = input(shape = (1, ))\n",
        "  #embedding for categorical input\n",
        "  li = Embedding(n_classes, 50)(in_label)\n",
        "  #Scale up to image dimensions with linear activation\n",
        "  n_nodes = in_shape[0]*in_shape[1]\n",
        "  li = Dense(n_nodes)(li)\n",
        "  #reshape to addtional channel\n",
        "  li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "  #Image input\n",
        "  in_image = Input(shape = in_shape)\n",
        "  #Concat label as channel\n",
        "  merge = Concatenate()([in_image, li])\n",
        "  #Downsample\n",
        "  fe = Conv2D(128, (3,3), strides = (2,2), padding = 'same')(merge)\n",
        "  fe = LeakyReLU(alpha = 0.2)(fe)\n",
        "  #Downsample\n",
        "  fe = Conv2D(128, (3,3), strides = (2,2), padding = 'same')(fe)\n",
        "  fe = LeakyReLU(alpha = 0.2)(fe)\n",
        "  #Flatten Feature Maps\n",
        "  fe = Flatten() (fe)\n",
        "  fe = Dropout(0.4) (fe)\n",
        "  #Output Layer\n",
        "  out_layer = Dense(1, activation = 'sigmoid')(fe)\n",
        "  #Define Model\n",
        "  model = Model([in_image, in_label], out_layer)\n",
        "  #Compile Model\n",
        "  opt = Adam(lr = 0.0002, beta_1 = 0.5)\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "#Defining the standalone generator model\n",
        "def define_generator(latent_dim, n_classes = 10):\n",
        "  #Label Input\n",
        "  in_label = Input(shape = (1, ))\n",
        "  #Embedding for categorical input\n",
        "  li = Embedding(n_classes, 50)(in_label)\n",
        "  #Linear Multiplication\n",
        "  n_nodes = 7*7\n",
        "  li = Dense(n_nodes)(li)\n",
        "  #Reshape to additional channel\n",
        "  li = Reshape((7, 7, 1))(li)\n",
        "  #Image Generator Input\n",
        "  in_lat = Input(shape = (latent_dim, ))\n",
        "  #Foundation for 7x7 images\n",
        "  n_nodes = 128*7*7\n",
        "  gen = Dense(n_nodes)(in_lat)\n",
        "  gen = LeakyReLU(alpha = 0.2)(gen)\n",
        "  gen = Reshape((7, 7, 128))(gen)\n",
        "  #Merge image gen and label input\n",
        "  merge = Concatenate()([gen, li])\n",
        "  #Upsample to 14x14\n",
        "  gen = Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = 'same')(merge)\n",
        "  gen = LeakyReLU(alpha = 0.2)(gen)\n",
        "  #Upsample to 28x28\n",
        "  gen = Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = 'same')(gen)\n",
        "  gen = LeakyReLU(alpha = 0.2)(gen)\n",
        "  #Output\n",
        "  out_layer = Conv2D(1, (7, 7), activation = 'tanh', padding = 'same')(gen)\n",
        "  #Define Model\n",
        "  model = Model([in_lat, in_label], out_layer)\n",
        "\n",
        "  return model\n",
        "\n",
        "#Defining the combined Generator model as well as the Discriminator model\n",
        "def define_gan(g_model, d_model):\n",
        "  #Make weights in discriminator non trainable\n",
        "  d_model.trainable = False\n",
        "  #Get noise and label inputs from generator models\n",
        "  gen_noise, gen_label = g_model.input\n",
        "  #get image output from generator output\n",
        "  gen_output = g_model.output\n",
        "  #Connect Image output and label input\n",
        "  gan_output = d_model([gen_output, gen_label])\n",
        "  #Define GAN model as taking in noise and outputting a classification\n",
        "  model = Model([gen_noise, gen_label], gan_output)\n",
        "  #Compile Model\n",
        "  opt = Adam(lr = 0.0002, beta_1 = 0.5)\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer = opt)\n",
        "\n",
        "  return model\n",
        "\n",
        "#Defining a function to load real samples\n",
        "def load_real_samples():\n",
        "  #Load DataSet\n",
        "  (trainX, trainy), (_, _) = load_data()\n",
        "  #Expand to 3D\n",
        "  X = expand_dims(trainX, axis = -1)\n",
        "  #Convert from ints to floats\n",
        "  X = X.astype('float32')\n",
        "  #Scale from 0 to 255\n",
        "  X = (X - 127.5)/ 127.5\n",
        "  return [X, trainy]\n",
        "\n",
        "#Generate and select real samples only\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "  #Split into images and labels\n",
        "  images, labels = dataset\n",
        "  #Choose random instances\n",
        "  ix = randint(0, images.shape[0], n_samples)\n",
        "  #Select images and labels\n",
        "  X, labels = images[ix], labels[ix]\n",
        "  #Generate Class Labels\n",
        "  y = ones((n_samples, 1))\n",
        "  return [X, labels], y\n",
        "\n",
        "#Generate an array of randomly generated points for the selection of labels\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes = 10):\n",
        "  #Generate points in latent space\n",
        "  x_input = randn(latent_dim * n_samples)\n",
        "  #Reshape into a batch of inputs from the network\n",
        "  z_input = x_input.reshape(n_samples, latent_dim)\n",
        "  #generate labels\n",
        "  labels = randint(0, n_classes, n_samples)\n",
        "  return [z_input, labels]\n",
        "\n",
        "#Generate an array of fake generated points for training the discriminator\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "  #Generate points in latent space\n",
        "  z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "  #Predict Outputs\n",
        "  images = generator.predict([z_input, labels_input])\n",
        "  #Create class labels\n",
        "  y = zeros((n_samples, 1))\n",
        "  return [images, labels_input], y\n",
        "\n",
        "#Training the GAN model (discriminator and generator at the same time)\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs = 100, n_batch = 128):\n",
        "  bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "\n",
        "  #Manually iterate epochs\n",
        "  for i in range(n_epochs):\n",
        "    for j in range(bat_per_epo):\n",
        "      [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "      #Update discriminator model weights\n",
        "      d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "      #Generate fake samples\n",
        "      [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      #Update discriminator model weights\n",
        "      d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "      #Prepare points in latent space as input for generator\n",
        "      [z_input, labels_input] = generate_label_points(latent_dim, n_batch)\n",
        "      #Create inverted labels for fake points\n",
        "      y_gan = ones((n_batch, 1))\n",
        "      #Update generator via discrminator's error\n",
        "      g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "      #Summarize loss on this batch\n",
        "      print(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss)\n",
        "  #Save the model\n",
        "  g_model.save('cgan_generator.h5')\n",
        "\n",
        "#Using this model for conditional clothing generation\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "#Function to save the plot\n",
        "def save_plot(examples, n):\n",
        "  #plot images\n",
        "  for i in range(n*n):\n",
        "    #Define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    #Turn Off Axis\n",
        "    pyplot.axis('off')\n",
        "    #Plot Raw Pixel Data\n",
        "    pyplot.imshow(examples[i, :, :, 0], cmap = 'gray_r')\n",
        "  pyplot.show()\n",
        "\n",
        "#Load model\n",
        "model = load_model('cgan_generator.h5')\n",
        "\n",
        "#Generate Images\n",
        "latent_points, labels = generate_latent_points(100, 100)\n",
        "\n",
        "#Specify Labels\n",
        "labels = asarray([x for _ in range(10) for x in range(10)])\n",
        "\n",
        "#Generate Images\n",
        "X = model.predict([latent_points, labels])\n",
        "\n",
        "#Scale from [-1, 1] to [0, 1]\n",
        "X = (X + 1) / 2.0\n",
        "\n",
        "#Plot the result\n",
        "save_plot(X, 10)"
      ],
      "metadata": {
        "id": "Yyx_t5iOUfQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DeOldify\n",
        "#It modernizes old looking images\n",
        "\n",
        "!git clone https://github.com/jantic/DeOldify.git DeOldify\n",
        "%cd DeOldify\n",
        "\n",
        "#Donwloading the model\n",
        "!mkdir './models/'\n",
        "!wget https://data.deepai.org/deoldify/ColorizeAsrtistic_gen.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dITRKjYeHwVL",
        "outputId": "3e42ca91-7d09-4486-c3b9-1d88372a3b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeOldify'...\n",
            "remote: Enumerating objects: 2609, done.\u001b[K\n",
            "remote: Counting objects: 100% (263/263), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 2609 (delta 87), reused 209 (delta 70), pack-reused 2346\u001b[K\n",
            "Receiving objects: 100% (2609/2609), 69.71 MiB | 15.96 MiB/s, done.\n",
            "Resolving deltas: 100% (1170/1170), done.\n",
            "/content/DeOldify\n",
            "mkdir: cannot create directory ‘./models/’: File exists\n",
            "--2024-01-16 16:58:52--  https://data.deepai.org/deoldify/ColorizeAsrtistic_gen.pth\n",
            "Resolving data.deepai.org (data.deepai.org)... 169.150.207.213, 2400:52e0:1500::1020:1\n",
            "Connecting to data.deepai.org (data.deepai.org)|169.150.207.213|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-01-16 16:58:53 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using DeOldify\n",
        "\n",
        "!pip install ffmpeg\n",
        "!pip install yt_dlp\n",
        "import sys\n",
        "from deoldify import device\n",
        "from deoldify.device_id import DeviceId\n",
        "device.set(device = DeviceId.GPU0)\n",
        "\n",
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('GPU is not available.')\n",
        "else:\n",
        "  print('Using GPU')\n",
        "\n",
        "import fastai\n",
        "from deoldify.visualize import *\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category = UserWarning, message = 'Your set is empty')\n",
        "\n",
        "URL = 'https://www.thesprucepets.com/thmb/hxWjs7evF2hP1Fb1c1HAvRi_Rw0=/2765x0/filters:no_upscale():strip_icc()/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg'\n",
        "\n",
        "RENDER_FACTOR = 35\n",
        "WATERMARK = False\n",
        "\n",
        "colorizer = get_image_colorizer(artistic = True)\n",
        "\n",
        "image_path = colorizer.plot_transformed_image_from_url(url = URL, render_factor = RENDER_FACTOR, compare = True, watermarked = WATERMARK)\n",
        "show_image_in_notebook(image_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "gcu0NYLdM2_N",
        "outputId": "6d171456-f3b8-4f8e-ac15-1b788305a1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n!pip install ffmpeg\\n!pip install yt_dlp\\nimport sys\\nfrom deoldify import device\\nfrom deoldify.device_id import DeviceId\\ndevice.set(device = DeviceId.GPU0)\\n\\nimport torch\\n\\nif not torch.cuda.is_available():\\n  print('GPU is not available.')\\nelse:\\n  print('Using GPU')\\n\\nimport fastai\\nfrom deoldify.visualize import *\\nimport warnings\\n\\nwarnings.filterwarnings('ignore', category = UserWarning, message = 'Your set is empty')\\n\\nURL = 'https://www.thesprucepets.com/thmb/hxWjs7evF2hP1Fb1c1HAvRi_Rw0=/2765x0/filters:no_upscale():strip_icc()/chinese-dog-breeds-4797219-hero-2a1e9c5ed2c54d00aef75b05c5db399c.jpg'\\n\\nRENDER_FACTOR = 35\\nWATERMARK = False\\n\\ncolorizer = get_image_colorizer(artistic = True)\\n\\nimage_path = colorizer.plot_transformed_image_from_url(url = URL, render_factor = RENDER_FACTOR, compare = True, watermarked = WATERMARK)\\nshow_image_in_notebook(image_path)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading TabGan for generating fake tabular data\n",
        "!wget https://raw.githubusercontent.com/Diyago/GAN-for-tabular-data/master/requirements.txt\n",
        "!pip install -r requirements.txt\n",
        "!pip install tabgan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HAmY8OavRJsN",
        "outputId": "052d4db3-5911-4959-f72f-d25060270496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tabgan\n",
            "  Downloading tabgan-2.0.5-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tabgan) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tabgan) (1.23.5)\n",
            "Collecting category-encoders (from tabgan)\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from tabgan) (1.11.0+cu113)\n",
            "Requirement already satisfied: lightgbm>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from tabgan) (4.1.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from tabgan) (1.2.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from tabgan) (0.16.0+cu121)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from tabgan) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tabgan) (4.66.1)\n",
            "Collecting catboost (from tabgan)\n",
            "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (from tabgan) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm>=2.2.3->tabgan) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->tabgan) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->tabgan) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (4.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost->tabgan) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost->tabgan) (3.7.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost->tabgan) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost->tabgan) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tabgan) (2023.3.post1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category-encoders->tabgan) (0.14.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category-encoders->tabgan) (0.5.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->tabgan) (2.31.0)\n",
            "Collecting torch>=1.0 (from tabgan)\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->tabgan) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0->tabgan)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->tabgan) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0->tabgan)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category-encoders->tabgan) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->tabgan) (2.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost->tabgan) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost->tabgan) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost->tabgan) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost->tabgan) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost->tabgan) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost->tabgan) (8.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->tabgan) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->tabgan) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->tabgan) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->tabgan) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0->tabgan) (1.3.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, catboost, torch, category-encoders, tabgan\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "Successfully installed catboost-1.2.2 category-encoders-2.6.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 tabgan-2.0.5 torch-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using TabGan on data\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "df = pd.read_csv('/content/sample_data/california_housing_train.csv', na_values = ['?', 'NA'])\n",
        "cols_used = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
        "cols_trained = ['latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
        "\n",
        "df = df[cols_used]\n",
        "\n",
        "#Handle missing values\n",
        "df['latitude'] = df['latitude'].fillna(df['latitude'].mean())\n",
        "\n",
        "#Split into training and testing set\n",
        "df_x_train, df_x_test, df_y_train, df_y_test = train_test_split(df.drop('longitude', axis = 1), df['longitude'], test_size = 0.2, random_state = 42)\n",
        "\n",
        "#Create dataframe version of the tabular GAN\n",
        "df_x_test, df_y_test = df_x_test.reset_index(drop = True), df_y_test.reset_index(drop = True)\n",
        "df_y_train = pd.DataFrame(df_y_train)\n",
        "df_y_test = pd.DataFrame(df_y_test)\n",
        "\n",
        "#Pandas to Numpy\n",
        "x_train = df_x_train.values\n",
        "x_test = df_x_test.values\n",
        "y_train = df_y_train.values\n",
        "y_test = df_y_test.values\n",
        "\n",
        "#Build the Neural Network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim = x_train.shape[1], activation = 'relu'))\n",
        "model.add(Dense(25, activation = 'relu'))\n",
        "model.add(Dense(12, activation = 'relu'))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
        "\n",
        "monitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto', restore_best_weights = True)\n",
        "model.fit(x_train, y_train, validation_data = (x_test, y_test), callbacks = [monitor], verbose = 2, epochs = 100)\n",
        "\n",
        "#Checking the accuracy of the data\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred, y_test))\n",
        "print('Final Score: ', score)\n",
        "\n",
        "#Generating the values using GAN\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC-nuG8uSuCw",
        "outputId": "ca56028b-68cf-4043-c0b9-5c19433b34e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "425/425 - 4s - loss: 2361237.5000 - val_loss: 11716.0820 - 4s/epoch - 10ms/step\n",
            "Epoch 2/100\n",
            "425/425 - 1s - loss: 10650.3145 - val_loss: 9017.5195 - 1s/epoch - 3ms/step\n",
            "Epoch 3/100\n",
            "425/425 - 1s - loss: 8091.4678 - val_loss: 7334.4590 - 1s/epoch - 3ms/step\n",
            "Epoch 4/100\n",
            "425/425 - 1s - loss: 6218.7236 - val_loss: 4964.3735 - 1s/epoch - 3ms/step\n",
            "Epoch 5/100\n",
            "425/425 - 1s - loss: 4571.6099 - val_loss: 3497.1550 - 1s/epoch - 3ms/step\n",
            "Epoch 6/100\n",
            "425/425 - 2s - loss: 3134.8833 - val_loss: 2432.7029 - 2s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "425/425 - 1s - loss: 2785.7439 - val_loss: 2696.5864 - 1s/epoch - 3ms/step\n",
            "Epoch 8/100\n",
            "425/425 - 1s - loss: 3807.2700 - val_loss: 3812.5757 - 1s/epoch - 3ms/step\n",
            "Epoch 9/100\n",
            "425/425 - 1s - loss: 3044.0000 - val_loss: 14271.8291 - 1s/epoch - 3ms/step\n",
            "Epoch 10/100\n",
            "425/425 - 1s - loss: 3361.8511 - val_loss: 1788.6560 - 1s/epoch - 3ms/step\n",
            "Epoch 11/100\n",
            "425/425 - 1s - loss: 5306.9937 - val_loss: 1154.3905 - 1s/epoch - 3ms/step\n",
            "Epoch 12/100\n",
            "425/425 - 1s - loss: 12060.1562 - val_loss: 992.0748 - 1s/epoch - 3ms/step\n",
            "Epoch 13/100\n",
            "425/425 - 1s - loss: 5252.9673 - val_loss: 2204.2820 - 1s/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "425/425 - 1s - loss: 44721.1719 - val_loss: 2190.7883 - 1s/epoch - 3ms/step\n",
            "Epoch 15/100\n",
            "425/425 - 1s - loss: 1525.4716 - val_loss: 3721.7935 - 1s/epoch - 3ms/step\n",
            "Epoch 16/100\n",
            "425/425 - 1s - loss: 1692.5662 - val_loss: 1158.0669 - 1s/epoch - 3ms/step\n",
            "Epoch 17/100\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "425/425 - 2s - loss: 3591.1104 - val_loss: 1418.4233 - 2s/epoch - 4ms/step\n",
            "Epoch 17: early stopping\n",
            "107/107 [==============================] - 0s 2ms/step\n",
            "Final Score:  31.4972213237645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using StyleGAN with any available input\n",
        "\n",
        "!tensorflow_version 1.x\n",
        "\n",
        "!git clone https://github.com/NVlabs/steylegan2-ada.git\n",
        "\n",
        "import sys\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "from IPython.display import Image\n",
        "import matlplotlib.pyplot as plt\n",
        "import IPython.display\n",
        "\n",
        "sys.path.insert(0, '/content/sample_data')\n",
        "\n",
        "import dnnlib\n",
        "import dnn.tflib as tflib\n",
        "\n",
        "def seed2vec(Gs, seed):\n",
        "  rnd = np.random.RandomState(seed)\n",
        "  return rnd.randn(1, *Gs.input_shape[1: ])\n",
        "\n",
        "def init_random_state(Gs, seed):\n",
        "  rnd = np.random.RandomState(seed)\n",
        "  noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "  tflib.set_vars({var : rnd.randn(*var.shape.as_list()) for var in noise_vars})\n",
        "\n",
        "def display_image(data, scale):\n",
        "  img = PIL.Image.fromarray(data, 'RGB')\n",
        "  width, height = img.size\n",
        "\n",
        "  img = img.resize((int(width*scale), int(height*scale)), PIL.Image.ANTIALIAS)\n",
        "\n",
        "  display(img)\n",
        "\n",
        "def generate_image(Gs, z, truncation_psi):\n",
        "  #Render images from dlatents initialized from random seeds\n",
        "  Gs_kwargs = {'output_transfrom' : dict(func = tflib.convert_images_to_uint8, nchw_to_nhcw = True), 'randomize_noise': False}\n",
        "\n",
        "  if truncation_psi in not None:\n",
        "    Gs_kwargs['truncation_psi'] = truncation_psi\n",
        "\n",
        "  label = np.zeros([1], Gs.input_shapes[1][1:])\n",
        "  images = Gs.run(z, label, **Gs_kwargs)\n",
        "  return images[0]\n",
        "\n",
        "URL = 'https://github.com/jeffheaton/pretrained-gen-fish/releases/download/1.0.0/fish-gan-2020-12-09.pkl'\n",
        "tflib.init_tf()\n",
        "print('Loading networks from :' , URL)\n",
        "with dnnlib.util.open_url(URL) as fp:\n",
        "  _g, _d, Gs = pickle.load(fp)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "#Choose starting and ending seed\n",
        "SEED_FROM = 1000\n",
        "SEED_TO = 2000\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!mkdir /content/\n",
        "\n",
        "#Generate the images from the seeds\n",
        "for i in tqdm(range(SEED_FROM, SEED_TO)):\n",
        "  init_random_state(Gs, 10)\n",
        "  z = seed2vec(Gs, 1)\n",
        "  img = generate_image(Gs, z, 1.0)\n",
        "  PIL.Image.fromarray(img, 'RGB').save(f'/content/images/seed-{i}.jpg')\n",
        "\n",
        "!cp -r /content/images/ /content/MyDrive/data/gan/images/\n",
        "\n",
        "SEEDS = [3004, 3031, 3033, 3111, 3191, 3253]\n",
        "STEPS = 100\n",
        "\n",
        "#Remove any prior results\n",
        "!rm /content/results/*\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "os.makedirs('./results/', exist_ok = True)\n",
        "\n",
        "#Generate the images for the video\n",
        "idx = 0\n",
        "for i in range(len(SEEDS) - 1):\n",
        "  v1 = seed2vec(Gs, SEEDS[i])\n",
        "  v2 = seed2vec(Gs, SEEDS[i+1])\n",
        "\n",
        "  diff = v2 - v1\n",
        "  step = diff / STEPS\n",
        "  current = v1.copy()\n",
        "\n",
        "  for j in tqdm(range(STEPS), desc= f'Seed {SEEDS[i]}'):\n",
        "    current += step\n",
        "    init_random_state =(Gs, 10)\n",
        "    img = generate_image(Gs, current, 1.0)\n",
        "    PIL.Image.fromarray(img, 'RGB').save(f'./results/frame-{idx}.png')\n",
        "    idx += 1\n",
        "\n",
        "!ffmpeg -r 30 -i /content/results/frame-%d.png -vcodec mpeg4 -y movie.mpeg4\n",
        "\n",
        "files.download('movie.mp4')\n"
      ],
      "metadata": {
        "id": "wef323_CLBmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up a new environment when the training is needed for more than 24 hours\n",
        "\n",
        "#Please restart runtime after this step\n",
        "!pip install torch == 1.8.1 torchvision == 0.9.1\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja\n",
        "\n",
        "#To convert image to a style accepted by StyleGAN2\n",
        "!mkdir dataset\n",
        "!mkdir experiments\n",
        "\n",
        "!python /content/stylegan2-ada-pytorch/dataset_tool.py --source /content/sample_data--dest /content/dataset/fishes\n",
        "\n",
        "#Clean up the images\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "IMAGE_PATH = '/content/sample_data'\n",
        "files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n",
        "\n",
        "base_size = None\n",
        "for file in tqdm(files):\n",
        "  file2 = os.path.join(IMAGE_PATH, file)\n",
        "  img = Image.open(file2)\n",
        "  sz = image.size\n",
        "  if base_size and sz != base_size:\n",
        "    print('Inconsistent size: ', file2)\n",
        "  elif img.mode != 'RGB':\n",
        "    print('Inconsistent color format: ', file)\n",
        "  else:\n",
        "    base_size = sz\n",
        "\n",
        "#To start training\n",
        "import os\n",
        "\n",
        "#Modify to suit requrements\n",
        "EXPERIMENTS = '/content/experiments'\n",
        "DATA = '/content/dataset/fishes'\n",
        "\n",
        "SNAP = 10\n",
        "\n",
        "#Build the command and run it\n",
        "cmd = f'/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --outdir {EXPERIMENTS} --data {DATA}'\n",
        "!{cmd}\n",
        "\n",
        "#Resume Training\n",
        "import os\n",
        "\n",
        "#Modify to suit your needs\n",
        "EXPERIMENTS = '/content/experiments'\n",
        "NETWORK = 'network-snapshot-000100.pkl'\n",
        "RESUME = os.path.join(EXPERIMENTS, '00008-circuit-autol-resumecustom')\n",
        "DATA = '/content/dataset/fishes'\n",
        "SNAP = 10\n",
        "\n",
        "#Build the command and run it\n",
        "cmd = f'/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --outdir {EXPERIMENTS} --data {DATA}'\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "FsAJzbUnNDj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}